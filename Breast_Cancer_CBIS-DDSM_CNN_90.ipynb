{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b525a9f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import PIL\n",
    "import cv2\n",
    "import uuid\n",
    "import shutil\n",
    "import glob as gb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm  # Progress bar\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import keras\n",
    "from sklearn.utils import shuffle\n",
    "from keras.optimizers import Adam\n",
    "from keras.regularizers import l1_l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Input\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.layers import Conv2D, MaxPool2D, BatchNormalization\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "db91b9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "dicom_df = pd.read_csv(\"C:/Users/AIMVLab/Desktop/flodo/CNN_Breast Cancer/CBIS-DDSM/csv/dicom_info.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b582f346",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.129308...\n",
       "3     CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.381187...\n",
       "6     CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.153339...\n",
       "7     CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.178994...\n",
       "10    CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.411833...\n",
       "Name: image_path, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cropped_images = dicom_df[dicom_df.SeriesDescription==\"cropped images\"].image_path\n",
    "cropped_images.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "872a14d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1     CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.248386...\n",
       "2     CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.267213...\n",
       "11    CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.210396...\n",
       "12    CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.749566...\n",
       "15    CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.987658...\n",
       "Name: image_path, dtype: object"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_mammogram = dicom_df[dicom_df.SeriesDescription==\"full mammogram images\"].image_path\n",
    "full_mammogram.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3082d595",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5     CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.153339...\n",
       "8     CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.178994...\n",
       "9     CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.411833...\n",
       "14    CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.236373...\n",
       "20    CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.357008...\n",
       "Name: image_path, dtype: object"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roi_mask = dicom_df[dicom_df.SeriesDescription==\"ROI mask images\"].image_path\n",
    "roi_mask.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d8dc6a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_path(sample, old_path, new_path):\n",
    "    return sample.replace(old_path, new_path, regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "addfdb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_smaples(sample, row=15, col=15):\n",
    "    plt.figure(figsize=(row, col))\n",
    "    for i, file in enumerate(sample[0:5]):\n",
    "        cropped_images_show = PIL.Image.open(file)\n",
    "        gray_img= cropped_images_show.convert(\"L\")\n",
    "        plt.subplot(1,5,i+1)\n",
    "        plt.imshow(gray_img, cmap='gray')\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d434d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_dir = \"C:/Users/AIMVLab/Desktop/flodo/CNN_Breast Cancer/CBIS-DDSM/jpeg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "238b73ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cropped Images paths:\n",
      "C:/Users/AIMVLab/Desktop/flodo/CNN_Breast Cancer/CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.129308726812851964007517874181459556304/1-172.jpg\n"
     ]
    }
   ],
   "source": [
    "cropped_images = replace_path(cropped_images, \"CBIS-DDSM/jpeg\", correct_dir)\n",
    "print('Cropped Images paths:')\n",
    "print(cropped_images.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "945efecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full mammo Images paths:\n",
      "C:/Users/AIMVLab/Desktop/flodo/CNN_Breast Cancer/CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.248386742010678582309005372213277814849/1-249.jpg\n"
     ]
    }
   ],
   "source": [
    "full_mammogram = replace_path(full_mammogram, \"CBIS-DDSM/jpeg\", correct_dir)\n",
    "print('\\nFull mammo Images paths:')\n",
    "print(full_mammogram.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cee89aa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ROI Mask Images paths:\n",
      "C:/Users/AIMVLab/Desktop/flodo/CNN_Breast Cancer/CBIS-DDSM/jpeg/1.3.6.1.4.1.9590.100.1.2.153339052913121382622526066491844156138/2-270.jpg\n"
     ]
    }
   ],
   "source": [
    "roi_mask = replace_path(roi_mask, \"CBIS-DDSM/jpeg\", correct_dir)\n",
    "print('\\nROI Mask Images paths:')\n",
    "print(roi_mask.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bcafe188",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_file_name(data, new_dict):\n",
    "    \"\"\"\n",
    "    /kaggle\n",
    "    /input\n",
    "    /cbis-ddsm-breast-cancer-image-dataset\n",
    "    /jpeg\n",
    "    /1.3.6.1.4.1.9590.100.1.2.129308726812851964007517874181459556304 [5]\n",
    "    /1-172.jpg\n",
    "    \n",
    "    return path at index [9] after split depends on split('\\')\n",
    "    \"\"\"\n",
    "    for dicom in data:\n",
    "        key = dicom.split('/')[8]\n",
    "#         print(key)\n",
    "        new_dict[key] = dicom\n",
    "    print(f\"the length of dataset ==> {len(new_dict.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9011e63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the length of dataset ==> 3567\n",
      "the length of dataset ==> 2857\n",
      "the length of dataset ==> 3247\n"
     ]
    }
   ],
   "source": [
    "cropped_images_dict = dict()\n",
    "full_mammo_dict = dict()\n",
    "roi_img_dict = dict()\n",
    "\n",
    "get_image_file_name(cropped_images, cropped_images_dict)\n",
    "get_image_file_name(full_mammogram, full_mammo_dict)\n",
    "get_image_file_name(roi_mask, roi_img_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c02fbf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_image_path(data):\n",
    "    \"\"\"Correct dicom paths to correct image paths.\"\"\"\n",
    "    for indx, image in enumerate(data.values):\n",
    "#         print(f\"Image Path: {image[11]}\")\n",
    "\n",
    "        img_name = image[11].split('/')[2]\n",
    "#         print(f\"Looking for key: {img_name}\")  # Debugging step\n",
    "\n",
    "        if img_name in full_mammo_dict:\n",
    "            data.iloc[indx, 11] = full_mammo_dict[img_name]\n",
    "        else:\n",
    "            data.iloc[indx, 11] = None\n",
    "#             print(f\"KeyError: '{img_name}' not found in full_mammo_dict\")  # Debugging step\n",
    "        \n",
    "        img_name = image[12].split('/')[2]\n",
    "        if img_name in cropped_images_dict:\n",
    "            data.iloc[indx, 12] = cropped_images_dict[img_name]\n",
    "        else:\n",
    "            data.iloc[indx, 12] = None\n",
    "#             print(f\"KeyError: '{img_name}' not found in cropped_images_dict\")  # Debugging step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1e1ea7c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_train = pd.read_csv(\"C:/Users/AIMVLab/Desktop/flodo/CNN_Breast Cancer/CBIS-DDSM/csv/mass_case_description_train_set.csv\")\n",
    "mass_test  = pd.read_csv(\"C:/Users/AIMVLab/Desktop/flodo/CNN_Breast Cancer/CBIS-DDSM/csv/mass_case_description_test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6112f792",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_image_path(mass_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "88d83817",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_train = mass_train.rename(columns={'left or right breast': 'left_or_right_breast',\n",
    "                                        'image view': 'image_view',\n",
    "                                        'abnormality id': 'abnormality_id',\n",
    "                                        'abnormality type': 'abnormality_type',\n",
    "                                        'mass shape': 'mass_shape',\n",
    "                                        'mass margins': 'mass_margins',\n",
    "                                        'image file path': 'image_file_path',\n",
    "                                        'cropped image file path': 'cropped_image_file_path',\n",
    "                                        'ROI mask file path': 'ROI_mask_file_path'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3e5a17ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_image_path(mass_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d7713f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mass_test = mass_test.rename(columns={'left or right breast': 'left_or_right_breast',\n",
    "                                      'image view': 'image_view',\n",
    "                                      'abnormality id': 'abnormality_id',\n",
    "                                      'abnormality type': 'abnormality_type',\n",
    "                                      'mass shape': 'mass_shape',\n",
    "                                      'mass margins': 'mass_margins',\n",
    "                                      'image file path': 'image_file_path',\n",
    "                                      'cropped image file path': 'cropped_image_file_path',\n",
    "                                      'ROI mask file path': 'ROI_mask_file_path'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "70013574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of mass_train: (1318, 14)\n",
      "Shape of mass_test: (378, 14)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of mass_train: {mass_train.shape}')\n",
    "print(f'Shape of mass_test: {mass_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "18f4dcb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_train = pd.read_csv(\"C:/Users/AIMVLab/Desktop/flodo/CNN_Breast Cancer/CBIS-DDSM/csv/calc_case_description_train_set.csv\")\n",
    "calc_test  = pd.read_csv(\"C:/Users/AIMVLab/Desktop/flodo/CNN_Breast Cancer/CBIS-DDSM/csv/calc_case_description_test_set.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "03e044d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_train = calc_train.rename(columns={'left or right breast': 'left_or_right_breast',\n",
    "                                        'image view': 'image_view',\n",
    "                                        'abnormality id': 'abnormality_id',\n",
    "                                        'abnormality type': 'abnormality_type',\n",
    "                                        'mass shape': 'mass_shape',\n",
    "                                        'mass margins': 'mass_margins',\n",
    "                                        'image file path': 'image_file_path',\n",
    "                                        'cropped image file path': 'cropped_image_file_path',\n",
    "                                        'ROI mask file path': 'ROI_mask_file_path'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e2a4409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_image_path(calc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7614b822",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_test = calc_test.rename(columns={'left or right breast': 'left_or_right_breast',\n",
    "                                      'image view': 'image_view',\n",
    "                                      'abnormality id': 'abnormality_id',\n",
    "                                      'abnormality type': 'abnormality_type',\n",
    "                                      'mass shape': 'mass_shape',\n",
    "                                      'mass margins': 'mass_margins',\n",
    "                                      'image file path': 'image_file_path',\n",
    "                                      'cropped image file path': 'cropped_image_file_path',\n",
    "                                      'ROI mask file path': 'ROI_mask_file_path'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "63fa1690",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_image_path(calc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cbd11fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of mass_train: (1546, 14)\n",
      "Shape of mass_test: (326, 14)\n"
     ]
    }
   ],
   "source": [
    "print(f'Shape of mass_train: {calc_train.shape}')\n",
    "print(f'Shape of mass_test: {calc_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c065e901",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = pd.concat([mass_train, mass_test, calc_train, calc_test], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2514e865",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_mapper = {'MALIGNANT': 1, 'BENIGN': 0, 'BENIGN_WITHOUT_CALLBACK': 0} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ae9c3723",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_size = (112, 112, 3) #112"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c1c18958",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AIMVLab\\AppData\\Local\\Temp\\ipykernel_56580\\614134976.py:2: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  full_dataset['labels'] = full_dataset['pathology'].replace(class_mapper).infer_objects(copy=False)\n"
     ]
    }
   ],
   "source": [
    "# Apply class mapper to pathology column\n",
    "full_dataset['labels'] = full_dataset['pathology'].replace(class_mapper).infer_objects(copy=False)\n",
    "\n",
    "full_images = np.array(full_dataset[full_dataset[\"image_file_path\"].notna()][\"image_file_path\"].tolist())\n",
    "full_labels = np.array(full_dataset[full_dataset[\"image_file_path\"].notna()][\"labels\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "260d3c43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_classes = len(full_dataset['labels'].unique())\n",
    "num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "15514d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['Benign', 'Malignant']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "dffd5244",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "labels\n",
      "0    2111\n",
      "1    1457\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Check the distribution of labels\n",
    "label_counts = full_dataset['labels'].value_counts()\n",
    "print(label_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "217d1f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images_with_unique_filenames(source_images, source_labels, destination_dir):\n",
    "    \"\"\"\n",
    "    Copy images from source to destination_dir in subfolders '0' and '1', ensuring unique filenames.\n",
    "    \"\"\"\n",
    "    benign_images = 0\n",
    "    malignant_images = 0\n",
    "    skipped_images = []\n",
    "\n",
    "    # Create the destination subfolders '0' and '1'\n",
    "    category_dest_dir_zero = os.path.join(destination_dir, '0')\n",
    "    os.makedirs(category_dest_dir_zero, exist_ok=True)\n",
    "    \n",
    "    category_dest_dir_one = os.path.join(destination_dir, '1')\n",
    "    os.makedirs(category_dest_dir_one, exist_ok=True)\n",
    "\n",
    "    # Copy all images to the appropriate folder based on the label\n",
    "    for idx, (source, label) in enumerate(zip(source_images, source_labels)):\n",
    "        if os.path.exists(source):\n",
    "            try:\n",
    "                # Generate a unique filename\n",
    "                filename = os.path.basename(source)\n",
    "                unique_filename = f\"{uuid.uuid4().hex}_{filename}\"\n",
    "                \n",
    "                if label == 0:\n",
    "                    shutil.copy(source, os.path.join(category_dest_dir_zero, unique_filename))\n",
    "                    benign_images += 1\n",
    "                elif label == 1:\n",
    "                    shutil.copy(source, os.path.join(category_dest_dir_one, unique_filename))\n",
    "                    malignant_images += 1\n",
    "                \n",
    "#                 print(f\"Copied image {idx + 1}/{len(source_images)} to class {label}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error copying image {source}: {e}\")\n",
    "                skipped_images.append(source)\n",
    "        else:\n",
    "            print(f\"Image not found: {source}\")\n",
    "            skipped_images.append(source)\n",
    "\n",
    "    # After running the function\n",
    "    print(f\"\\nCopying complete.\")\n",
    "    print(f\"Benign images copied (label 0): {benign_images}\")\n",
    "    print(f\"Malignant images copied (label 1): {malignant_images}\")\n",
    "    print(f\"Total skipped images: {len(skipped_images)}\")\n",
    "    if skipped_images:\n",
    "        print(\"Skipped images:\")\n",
    "        for img in skipped_images:\n",
    "            print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "ad9739ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Copying complete.\n",
      "Benign images copied (label 0): 1931\n",
      "Malignant images copied (label 1): 1355\n",
      "Total skipped images: 0\n"
     ]
    }
   ],
   "source": [
    "destination_dir = \"D:/OKPALA/CNN_Breast Cancer/CBIS-DDSM/marged_images\"  # Update this path as needed\n",
    "\n",
    "copy_images_with_unique_filenames(full_images, full_labels, destination_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2c02945e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_images_to_destination(source_dir, destination_dir):\n",
    "    \"\"\"\n",
    "    Move or copy images from source_dir to destination_dir in subfolders '0' and '1'.\n",
    "    \"\"\"\n",
    "    # Define the categories (class labels) as '0' and '1'\n",
    "    categories = ['0', '1']\n",
    "    \n",
    "    for category in categories:\n",
    "        # Create the destination subfolder (e.g., '0' or '1') if it doesn't exist\n",
    "        category_dest_dir = os.path.join(destination_dir, category)\n",
    "        os.makedirs(category_dest_dir, exist_ok=True)\n",
    "        \n",
    "        # Path to the current category folder in the source directory\n",
    "        category_source_dir = os.path.join(source_dir, category)\n",
    "        \n",
    "        # Move or copy all images from the source category folder to the destination folder\n",
    "        for img_name in os.listdir(category_source_dir):\n",
    "            # Construct full file paths\n",
    "            img_source_path = os.path.join(category_source_dir, img_name)\n",
    "            img_dest_path = os.path.join(category_dest_dir, img_name)\n",
    "            \n",
    "            # Copy the image to the destination folder (you can use shutil.move to move instead of copy)\n",
    "            shutil.copy(img_source_path, img_dest_path)\n",
    "            \n",
    "            # Optionally, print the status\n",
    "#             print(f\"Copied {img_source_path} to {img_dest_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "22131027",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the paths to your dataset folders\n",
    "test = \"D:/ASMA/CNN_Breast Cancer/archive/test\"\n",
    "train = \"D:/ASMA/CNN_Breast Cancer/archive/train\"\n",
    "valid = \"D:/ASMA/CNN_Breast Cancer/archive/valid\"\n",
    "\n",
    "# Destination directory for the unified dataset\n",
    "destination_dir = \"D:/ASMA/CNN_Breast Cancer/CBIS-DDSM/marged_images\"  # Update this path as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c03630a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each of the dataset folders (test, train, valid)\n",
    "copy_images_to_destination(test, destination_dir)\n",
    "copy_images_to_destination(train, destination_dir)\n",
    "copy_images_to_destination(valid, destination_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "af67a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_processor(image_path, target_size):\n",
    "    \"\"\"Preprocess images for CNN model\"\"\"\n",
    "    if image_path:\n",
    "#         absolute_image_path = os.path.abspath(image_path)\n",
    "        image = cv2.imread(image_path)\n",
    "        \n",
    "        # Check if the image was loaded successfully\n",
    "        if image is None:\n",
    "#             print(f\"Warning: Failed to load image at {absolute_image_path}. Skipping this file.\")\n",
    "            return None\n",
    "\n",
    "        # Resize the image and normalize it\n",
    "        image = cv2.resize(image, (target_size[0], target_size[1]))\n",
    "        image_array = image / 255.0  # Normalize the image to range [0, 1]\n",
    "        return image_array\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "4fb37b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_zero = 'D:/ASMA/CNN_Breast Cancer/CBIS-DDSM/marged_images/0'\n",
    "# path_one  = 'D:/ASMA/CNN_Breast Cancer/CBIS-DDSM/marged_images/1'\n",
    "\n",
    "# # Preprocess and collect images and labels\n",
    "# benign_images = np.array([image_processor(os.path.join(path_zero, x), (224, 224)) for x in os.listdir(path_zero)])\n",
    "# benign_labels = np.array([0] * len(benign_images))  # Label 0 for benign\n",
    "\n",
    "# malignant_images = np.array([image_processor(os.path.join(path_one, x), (224, 224)) for x in os.listdir(path_one)])\n",
    "# malignant_labels = np.array([1] * len(malignant_images))  # Label 1 for malignant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3b654827",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def image_processor(image_path, target_size):\n",
    "    \"\"\"Preprocess images for CNN model.\"\"\"\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # Check if the image was loaded successfully\n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    # Resize and normalize the image\n",
    "    image = cv2.resize(image, target_size)\n",
    "    return image\n",
    "\n",
    "def load_images_from_directory(directory, target_size, label):\n",
    "    \"\"\"Load and preprocess images from a directory.\"\"\"\n",
    "    image_files = [entry.path for entry in os.scandir(directory) if entry.is_file()]\n",
    "    \n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        images = list(executor.map(lambda img: image_processor(img, target_size), image_files))\n",
    "    \n",
    "    # Filter out None values in case some images failed to load\n",
    "    images = np.array([img for img in images if img is not None])\n",
    "    labels = np.full(len(images), label)\n",
    "    \n",
    "    return images, labels\n",
    "\n",
    "# Define paths and target size\n",
    "path_zero = 'D:/ASMA/CNN_Breast Cancer/CBIS-DDSM/marged_images/0'\n",
    "path_one  = 'D:/ASMA/CNN_Breast Cancer/CBIS-DDSM/marged_images/1'\n",
    "target_size = (112, 112)\n",
    "\n",
    "# Load benign and malignant images\n",
    "benign_images, benign_labels = load_images_from_directory(path_zero, target_size, label=0)\n",
    "malignant_images, malignant_labels = load_images_from_directory(path_one, target_size, label=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "2c76a4c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Count of 0 is: (29259, 112, 112, 3)\n",
      "\n",
      "The count of 0 label is: (29259,)\n",
      "\n",
      "The count of 1 is: (20128, 112, 112, 3)\n",
      "\n",
      "The count of 1 label is: (20128,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The Count of 0 is: {benign_images.shape}\")\n",
    "print()\n",
    "print(f\"The count of 0 label is: {benign_labels.shape}\")\n",
    "print()\n",
    "print(f\"The count of 1 is: {malignant_images.shape}\")\n",
    "print()\n",
    "print(f\"The count of 1 label is: {malignant_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "9a683c1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing complete. Images and labels combined.\n"
     ]
    }
   ],
   "source": [
    "# Concatenate images and labels\n",
    "full_images = np.concatenate([benign_images, malignant_images])\n",
    "full_labels = np.concatenate([benign_labels, malignant_labels])\n",
    "print(\"Preprocessing complete. Images and labels combined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f5019007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the dataset (images and labels in unison)\n",
    "full_images, full_labels = shuffle(full_images, full_labels, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "544b6cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images, valid_images, train_labels, valid_labels = train_test_split(full_images, full_labels,test_size=0.2,random_state=40)\n",
    "valid_images, test_images, valid_labels, test_labels = train_test_split(valid_images, valid_labels,test_size=0.5,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0d3c0632",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "(39509, 112, 112, 3)\n",
      "(39509,)\n",
      "Valid:\n",
      "(4939, 112, 112, 3)\n",
      "(4939,)\n",
      "Test:\n",
      "(4939, 112, 112, 3)\n",
      "(4939,)\n"
     ]
    }
   ],
   "source": [
    "print(\"Train:\")\n",
    "print(train_images.shape)\n",
    "print(train_labels.shape)\n",
    "print(\"Valid:\")\n",
    "print(valid_images.shape)\n",
    "print(valid_labels.shape)\n",
    "print(\"Test:\")\n",
    "print(test_images.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8445fef8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(train_images[10])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2440b60d",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 2\n",
    "\n",
    "train_labels = to_categorical(train_labels, num_classes)\n",
    "test_labels  = to_categorical(test_labels, num_classes)\n",
    "valid_labels = to_categorical(valid_labels, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ccea5a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 112, 112, 3)]     0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 112, 112, 32)      896       \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 112, 112, 32)      9248      \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 112, 112, 32)     128       \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 56, 56, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 56, 56, 32)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 56, 56, 64)        18496     \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 56, 56, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 56, 56, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 28, 28, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 28, 28, 64)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 28, 28, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 28, 28, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 28, 28, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 14, 14, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 14, 14, 128)       0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 14, 14, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 14, 14, 128)      512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 7, 7, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 7, 7, 128)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 6272)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               802944    \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 258       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,386,786\n",
      "Trainable params: 1,386,082\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "#Model Archiecture\n",
    "# num_classes = y_train.shape[1] * y_train.shape[2]\n",
    "#input layer\n",
    "inputs = tf.keras.Input(shape=(train_images.shape[1], train_images.shape[2], train_images.shape[3]))\n",
    "x = inputs\n",
    "\n",
    "# Convolutional Block\n",
    "for filters in [32, 64, 128, 128]: \n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=3, padding=\"same\", activation=\"relu\",)(x)\n",
    "    x = tf.keras.layers.Conv2D(filters, kernel_size=3, padding=\"same\", activation=\"relu\",)(x)\n",
    "    x = tf.keras.layers.BatchNormalization()(x)\n",
    "    x = tf.keras.layers.MaxPooling2D(pool_size=(2,2))(x)\n",
    "    x = tf.keras.layers.Dropout(0.2)(x)\n",
    "    \n",
    "#fully Connected layer\n",
    "x = tf.keras.layers.Flatten()(x)\n",
    "# x = tf.keras.layers.GlobalMaxPooling1D()(x)\n",
    "# x = tf.keras.layers.Dense(128, activation=\"relu\", kernel_regularizer=l2(0.03))(x)\n",
    "# x = tf.keras.layers.Dropout(0.5)(x)\n",
    "x = tf.keras.layers.Dense(128, activation=\"relu\")(x)\n",
    "x = tf.keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "#output layer\n",
    "outputs = tf.keras.layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = tf.keras.models.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ec610ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001), loss=tf.losses.BinaryCrossentropy(), metrics=[\"accuracy\",\"top_k_categorical_accuracy\"])\n",
    "\n",
    "model_folder =\"D:\\\\ASMA\\\\trained_model\"\n",
    "# csv_path = os.path.join(\"files\", \"log.csv\")\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "# Define the callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(f'{model_folder}/Breast_cancer_model_{timestamp}.keras', monitor=\"val_loss\", verbose=1, save_best_only=True),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor=\"val_loss\", factor=0.1, patience=5, min_lr=1e-7),\n",
    "    # tf.keras.callbacks.CSVLogger(csv_path),\n",
    "    tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=10, restore_best_weights=False)\n",
    "]\n",
    "\n",
    "# # Define the number of validation steps\n",
    "# steps_per_epoch = len(image_paths_train) // BATCH_SIZE\n",
    "# validation_steps = len(image_paths_val) // BATCH_SIZE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "d45dcf93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "1235/1235 [==============================] - ETA: 0s - loss: 0.6370 - accuracy: 0.6275 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 1: val_loss improved from inf to 0.83028, saving model to D:\\OKPALA\\trained_model\\Breast_cancer_model_20241017-072527.keras\n",
      "1235/1235 [==============================] - 37s 20ms/step - loss: 0.6370 - accuracy: 0.6275 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.8303 - val_accuracy: 0.6404 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 2/1000\n",
      "1235/1235 [==============================] - ETA: 0s - loss: 0.5375 - accuracy: 0.7005 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 2: val_loss improved from 0.83028 to 0.78684, saving model to D:\\OKPALA\\trained_model\\Breast_cancer_model_20241017-072527.keras\n",
      "1235/1235 [==============================] - 23s 19ms/step - loss: 0.5375 - accuracy: 0.7005 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.7868 - val_accuracy: 0.6878 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 3/1000\n",
      "1234/1235 [============================>.] - ETA: 0s - loss: 0.4714 - accuracy: 0.7510 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 3: val_loss improved from 0.78684 to 0.54689, saving model to D:\\OKPALA\\trained_model\\Breast_cancer_model_20241017-072527.keras\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.4715 - accuracy: 0.7509 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.5469 - val_accuracy: 0.7423 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 4/1000\n",
      "1232/1235 [============================>.] - ETA: 0s - loss: 0.4003 - accuracy: 0.7986 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 4: val_loss improved from 0.54689 to 0.46237, saving model to D:\\OKPALA\\trained_model\\Breast_cancer_model_20241017-072527.keras\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.4005 - accuracy: 0.7985 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.4624 - val_accuracy: 0.7842 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 5/1000\n",
      "1234/1235 [============================>.] - ETA: 0s - loss: 0.3272 - accuracy: 0.8429 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 5: val_loss improved from 0.46237 to 0.24761, saving model to D:\\OKPALA\\trained_model\\Breast_cancer_model_20241017-072527.keras\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.3272 - accuracy: 0.8429 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2476 - val_accuracy: 0.8957 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 6/1000\n",
      "1234/1235 [============================>.] - ETA: 0s - loss: 0.2580 - accuracy: 0.8826 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 6: val_loss improved from 0.24761 to 0.21589, saving model to D:\\OKPALA\\trained_model\\Breast_cancer_model_20241017-072527.keras\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.2580 - accuracy: 0.8827 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.2159 - val_accuracy: 0.9075 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 7/1000\n",
      "1232/1235 [============================>.] - ETA: 0s - loss: 0.2030 - accuracy: 0.9114 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 7: val_loss improved from 0.21589 to 0.10461, saving model to D:\\OKPALA\\trained_model\\Breast_cancer_model_20241017-072527.keras\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.2029 - accuracy: 0.9115 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1046 - val_accuracy: 0.9623 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 8/1000\n",
      "1235/1235 [==============================] - ETA: 0s - loss: 0.1675 - accuracy: 0.9271 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 8: val_loss did not improve from 0.10461\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.1675 - accuracy: 0.9271 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1345 - val_accuracy: 0.9451 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 9/1000\n",
      "1234/1235 [============================>.] - ETA: 0s - loss: 0.1377 - accuracy: 0.9405 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 9: val_loss did not improve from 0.10461\n",
      "1235/1235 [==============================] - 23s 19ms/step - loss: 0.1377 - accuracy: 0.9405 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.1289 - val_accuracy: 0.9476 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 10/1000\n",
      "1234/1235 [============================>.] - ETA: 0s - loss: 0.1209 - accuracy: 0.9473 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 10: val_loss improved from 0.10461 to 0.07764, saving model to D:\\OKPALA\\trained_model\\Breast_cancer_model_20241017-072527.keras\n",
      "1235/1235 [==============================] - 23s 19ms/step - loss: 0.1209 - accuracy: 0.9473 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0776 - val_accuracy: 0.9678 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 11/1000\n",
      "1234/1235 [============================>.] - ETA: 0s - loss: 0.1071 - accuracy: 0.9517 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 11: val_loss improved from 0.07764 to 0.07165, saving model to D:\\OKPALA\\trained_model\\Breast_cancer_model_20241017-072527.keras\n",
      "1235/1235 [==============================] - 23s 19ms/step - loss: 0.1071 - accuracy: 0.9517 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0717 - val_accuracy: 0.9692 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 12/1000\n",
      "1232/1235 [============================>.] - ETA: 0s - loss: 0.1015 - accuracy: 0.9534 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 12: val_loss improved from 0.07165 to 0.06894, saving model to D:\\OKPALA\\trained_model\\Breast_cancer_model_20241017-072527.keras\n",
      "1235/1235 [==============================] - 23s 19ms/step - loss: 0.1015 - accuracy: 0.9534 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0689 - val_accuracy: 0.9696 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 13/1000\n",
      "1234/1235 [============================>.] - ETA: 0s - loss: 0.0908 - accuracy: 0.9576 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 13: val_loss did not improve from 0.06894\n",
      "1235/1235 [==============================] - 23s 19ms/step - loss: 0.0908 - accuracy: 0.9576 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0796 - val_accuracy: 0.9656 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 14/1000\n",
      "1232/1235 [============================>.] - ETA: 0s - loss: 0.0871 - accuracy: 0.9599 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 14: val_loss improved from 0.06894 to 0.06660, saving model to D:\\OKPALA\\trained_model\\Breast_cancer_model_20241017-072527.keras\n",
      "1235/1235 [==============================] - 23s 19ms/step - loss: 0.0871 - accuracy: 0.9600 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0666 - val_accuracy: 0.9690 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 15/1000\n",
      "1233/1235 [============================>.] - ETA: 0s - loss: 0.0842 - accuracy: 0.9615 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 15: val_loss did not improve from 0.06660\n",
      "1235/1235 [==============================] - 23s 19ms/step - loss: 0.0841 - accuracy: 0.9616 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0676 - val_accuracy: 0.9684 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 16/1000\n",
      "1232/1235 [============================>.] - ETA: 0s - loss: 0.0797 - accuracy: 0.9619 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 16: val_loss did not improve from 0.06660\n",
      "1235/1235 [==============================] - 23s 19ms/step - loss: 0.0798 - accuracy: 0.9619 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0682 - val_accuracy: 0.9684 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 17/1000\n",
      "1233/1235 [============================>.] - ETA: 0s - loss: 0.0758 - accuracy: 0.9641 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 17: val_loss improved from 0.06660 to 0.05955, saving model to D:\\OKPALA\\trained_model\\Breast_cancer_model_20241017-072527.keras\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.0759 - accuracy: 0.9640 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0595 - val_accuracy: 0.9694 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 18/1000\n",
      "1232/1235 [============================>.] - ETA: 0s - loss: 0.0735 - accuracy: 0.9638 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 18: val_loss improved from 0.05955 to 0.05790, saving model to D:\\OKPALA\\trained_model\\Breast_cancer_model_20241017-072527.keras\n",
      "1235/1235 [==============================] - 25s 20ms/step - loss: 0.0737 - accuracy: 0.9637 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0579 - val_accuracy: 0.9698 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 19/1000\n",
      "1234/1235 [============================>.] - ETA: 0s - loss: 0.0734 - accuracy: 0.9646 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 19: val_loss did not improve from 0.05790\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.0734 - accuracy: 0.9646 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0662 - val_accuracy: 0.9696 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 20/1000\n",
      "1232/1235 [============================>.] - ETA: 0s - loss: 0.0711 - accuracy: 0.9648 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 20: val_loss did not improve from 0.05790\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.0710 - accuracy: 0.9648 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0626 - val_accuracy: 0.9674 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 21/1000\n",
      "1235/1235 [==============================] - ETA: 0s - loss: 0.0683 - accuracy: 0.9669 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 21: val_loss did not improve from 0.05790\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.0683 - accuracy: 0.9669 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0705 - val_accuracy: 0.9682 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 22/1000\n",
      "1233/1235 [============================>.] - ETA: 0s - loss: 0.0666 - accuracy: 0.9674 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 22: val_loss did not improve from 0.05790\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.0666 - accuracy: 0.9675 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0685 - val_accuracy: 0.9692 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 23/1000\n",
      "1234/1235 [============================>.] - ETA: 0s - loss: 0.0655 - accuracy: 0.9676 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 23: val_loss did not improve from 0.05790\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.0654 - accuracy: 0.9676 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0652 - val_accuracy: 0.9702 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-04\n",
      "Epoch 24/1000\n",
      "1234/1235 [============================>.] - ETA: 0s - loss: 0.0556 - accuracy: 0.9702 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 24: val_loss did not improve from 0.05790\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.0556 - accuracy: 0.9703 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0648 - val_accuracy: 0.9696 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 25/1000\n",
      "1234/1235 [============================>.] - ETA: 0s - loss: 0.0511 - accuracy: 0.9725 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 25: val_loss did not improve from 0.05790\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.0511 - accuracy: 0.9725 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0673 - val_accuracy: 0.9694 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 26/1000\n",
      "1233/1235 [============================>.] - ETA: 0s - loss: 0.0490 - accuracy: 0.9732 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 26: val_loss did not improve from 0.05790\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.0490 - accuracy: 0.9732 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0683 - val_accuracy: 0.9696 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 27/1000\n",
      "1233/1235 [============================>.] - ETA: 0s - loss: 0.0481 - accuracy: 0.9734 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 27: val_loss did not improve from 0.05790\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.0480 - accuracy: 0.9734 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0696 - val_accuracy: 0.9702 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Epoch 28/1000\n",
      "1235/1235 [==============================] - ETA: 0s - loss: 0.0462 - accuracy: 0.9748 - top_k_categorical_accuracy: 1.0000\n",
      "Epoch 28: val_loss did not improve from 0.05790\n",
      "1235/1235 [==============================] - 24s 19ms/step - loss: 0.0462 - accuracy: 0.9748 - top_k_categorical_accuracy: 1.0000 - val_loss: 0.0704 - val_accuracy: 0.9704 - val_top_k_categorical_accuracy: 1.0000 - lr: 1.0000e-05\n",
      "Training Time: 0 hours, 11 minutes, and 15 seconds.\n",
      "This is the End!!!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Training the model\n",
    "history = model.fit( \n",
    "    train_images, \n",
    "    train_labels,\n",
    "    # steps_per_epoch=steps_per_epoch,\n",
    "    epochs=1000,\n",
    "    validation_data=(valid_images, valid_labels),\n",
    "    # validation_steps=validation_steps,\n",
    "    callbacks=callbacks,\n",
    "    # class_weight = class_weights\n",
    ")\n",
    "\n",
    "# Stop the timer after training completes\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate the total training time\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# Print the total training time in a human-readable format (hours, minutes, seconds)\n",
    "hours, rem = divmod(training_time, 3600)\n",
    "minutes, seconds = divmod(rem, 60)\n",
    "print(f\"Training Time: {int(hours)} hours, {int(minutes)} minutes, and {int(seconds)} seconds.\")\n",
    "\n",
    "print(\"This is the End!!!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "e8cff7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155/155 [==============================] - 1s 6ms/step - loss: 0.0732 - accuracy: 0.9654 - top_k_categorical_accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.07318150997161865, 0.9653776288032532, 1.0]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed8e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('Model Accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d08229d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa47c2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Predict using the trained model\n",
    "Y_pred = model.predict(test_images,test_labels) steps=len(test_images))\n",
    "Y_pred_classes = np.argmax(Y_pred , axis=1)\n",
    "\n",
    "# Extract true labels from the test generator\n",
    "# This assumes that test_datagen is correctly set up with y_test\n",
    "Y_true = np.argmax(test_labels), axis=1)  # Modify this if your setup differs\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion_mtx = confusion_matrix(Y_true, Y_pred_classes)\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = sns.heatmap(confusion_mtx, annot=True, fmt=\"d\", linewidths=.5, cmap=\"Blues\")\n",
    "ax.set_xlabel('Predicted Labels')\n",
    "ax.set_ylabel('True Labels')\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.xaxis.set_ticklabels(['Non-Cancer', 'Cancer'])\n",
    "ax.yaxis.set_ticklabels(['Non-Cancer', 'Cancer'])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07083493",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming Y_pred and Y_true are already defined\n",
    "# Y_pred is the output from model.predict, Y_pred_classes is obtained by np.argmax(Y_pred, axis=1)\n",
    "Y_true = np.argmax(test_labels, axis=1)  # Make sure y_test is correctly prepared\n",
    "\n",
    "# Calculate Precision, Recall, and F1 Score\n",
    "precision = precision_score(Y_true, Y_pred_classes)\n",
    "recall = recall_score(Y_true, Y_pred_classes)\n",
    "f1 = f1_score(Y_true, Y_pred_classes)\n",
    "\n",
    "# Calculate the probabilities for the positive class\n",
    "Y_pred_probs = Y_pred[:, 1]  # assuming your model outputs probabilities for each class\n",
    "\n",
    "# ROC AUC Score\n",
    "roc_auc = roc_auc_score(Y_true, Y_pred_probs)\n",
    "\n",
    "# Print the metrics\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n",
    "print(f\"ROC AUC Score: {roc_auc:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f27ce1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot ROC Curve\n",
    "fpr, tpr, _ = roc_curve(Y_true, Y_pred_probs)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cb01fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define a simple model\n",
    "model = Sequential([\n",
    "    Dense(64, activation='relu', input_shape=(32,)),\n",
    "    Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Save the model\n",
    "model.save('CBIS-DDSM_CNN.h5')\n",
    "\n",
    "\n",
    "#model.save('CBS_Update.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3d0995",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GPU",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
